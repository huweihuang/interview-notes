{"./":{"url":"./","title":"序言","keywords":"","body":"面试指南 Linux基础 Linux基础 数据结构 数据结构基础 TCP/IP TCP/IP基础 Golang Golang基础 Golang进阶 Kubernetes k8s基础 k8s进阶 算法题 LeetCode Copyright © www.huweihuang.com all right reserved，powered by GitbookUpdated at 2025-07-08 20:55:40 "},"linux/linux.html":{"url":"linux/linux.html","title":"Linux基础","keywords":"","body":"TCP 三次握手 第一次握手：A发一个syn包，带seq num给B 第二次握手：B返回一个ack包 seq num+1 给A，同时给A发了个自己的syn包，带seq num 第三次握手：A返回一个ack包 seq num+1给B。 为什么需要三次握手，假如没有第三次握手，此时A断连了，而B不知道就开始向A传输数据，则会造成数据丢失，且B不知道而频繁发送。 四次挥手 第一次挥手：A发一个FIN包，带seq num给B（A主动发起关闭） 第二次挥手：B返回一个ack seq num+1 给A（B同意A的关闭） 第三次挥手：B发一个FIN包，带seq num给A （B发起关闭） 第四次挥手：A返回一个ack,seq num+1给B （A同意B的关闭） 为什么要有四次握手，主要是因为TCP是半关闭的，即关闭的单向的，当A向B的通道关闭后，B仍然可以向A发送消息，当B也关闭了A的通道，双方之间才算正式关闭，不能互相发送消息。 连接 长连接 client向server发起连接，server接受client连接，双方建立连接。Client与server完成一次读写之后，它们之间的连接并不会主动关闭，后续的读写操作会继续使用这个连接。 短连接 client向server发起连接请求，server接到请求，然后双方建立连接。client向server发送消息，server回应client，然后一次读写就完成了，这时候双方任何一个都可以发起close操作，不过一般都是client先发起close操作。 http/https Https的原理就是：在传输层和应用层之间加了一层SSL/TLS。Https指的是Http协议+ SSL/TLS协议,也就是说，Http协议本来定义了数据的包装方式（要包含请求头，请求方式等），现在在这个基础上，还要求行对数据进行加密，对通信双方身份进行验证（如何加密，如何验证由SSL决定）。 keepalived的原理 首先，每个节点有一个初始优先级，由配置文件中的priority配置项指定，MASTER节点的priority应比BAKCUP高。运行过程中keepalived根据vrrp_script的weight设定，增加或减小节点优先级。规则如下： “weight”值为正时,脚本检测成功时”weight”值会加到”priority”上,检测失败是不加 主失败: 主priority 主成功: 主priority+weight之和>备priority+weight之和时,主依然为主,即不发生切换 “weight”为负数时,脚本检测成功时”weight”不影响”priority”,检测失败时,Master节点的权值将是“priority“值与“weight”值之差 主失败: 主priotity-abs(weight) 主成功: 主priority > 备priority 不切换 进程、线程、协程 进程 进程是资源分配的单位，即资源隔离。同进程内的所有线程可以共同访问该进程的资源，每个进程都有自己的独立内存空间，不同进程通过进程间通信来通信。由于进程比较重量，占据独立的内存，所以上下文进程间的切换开销（栈、寄存器、虚拟内存、文件句柄等）比较大，但相对比较稳定安全。 线程 线程是独立调度和运行的基本单位，线程间通信主要通过共享内存，上下文切换很快，资源开销较少，但相比进程不够稳定容易丢失数据。 协程 协程是一种用户态的轻量级线程，协程的调度完全由用户控制。协程没有线程的上下文切换消耗， 乐观锁和悲观锁 乐观锁（Optimistic Lock） 和 悲观锁（Pessimistic Lock） 是两种并发控制策略，用于解决并发访问共享资源时的数据一致性问题。 特性 乐观锁 悲观锁 定义 假设大多数情况下不会发生冲突，在更新时才检测冲突 假设会发生冲突，访问资源前先加锁 实现方式 通常基于 版本号（Version） 或 时间戳 基于 加锁机制（如互斥锁、数据库锁） 性能 无锁机制，适合读多写少的场景 有锁机制，适合写多冲突多的场景 代价 需要重试机制（例如 CAS 重试） 上锁/阻塞带来额外性能消耗 举例 数据库中通过版本号字段控制更新 Synchronized、mutex、数据库 select for update 等 Kubernetes 使用哪种锁？ Kubernetes 大量使用的是“乐观锁”机制。 为什么？ 因为： Kubernetes 中的资源对象（如 Pod、Deployment）是以声明式方式进行修改，用户/控制器常常会并发更新同一个资源； 它依赖 etcd 存储，etcd 自身支持 基于版本号的乐观并发控制（Optimistic Concurrency Control）； 提高性能，避免频繁阻塞。 K8s 中乐观锁的实现方式 在 Kubernetes 中，资源对象的每次更新都会携带一个字段： metadata: resourceVersion: \"123456\" 工作流程： 客户端通过 API Server 获取资源对象，拿到当前的 resourceVersion。 客户端修改对象并发出更新请求，带上这个 resourceVersion。 API Server 将此 resourceVersion 与 etcd 中的最新版本进行比较： 如果一致，则允许更新。 如果不一致，说明资源已被其他人修改，返回 409 Conflict。 客户端收到冲突错误，可进行重试逻辑（常用于 controller 中）。 这就是典型的 乐观锁机制。 Copyright © www.huweihuang.com all right reserved，powered by GitbookUpdated at 2025-07-08 20:55:40 "},"data-structure/data-structure.html":{"url":"data-structure/data-structure.html","title":"数据结构基础","keywords":"","body":"Copyright © www.huweihuang.com all right reserved，powered by GitbookUpdated at 2025-07-08 20:55:40 "},"tcpip/tcpip.html":{"url":"tcpip/tcpip.html","title":"TCP/IP基础","keywords":"","body":"Copyright © www.huweihuang.com all right reserved，powered by GitbookUpdated at 2025-07-08 20:55:40 "},"golang/golang-basic.html":{"url":"golang/golang-basic.html","title":"Golang基础","keywords":"","body":"Copyright © www.huweihuang.com all right reserved，powered by GitbookUpdated at 2025-07-08 20:55:40 "},"golang/golang-advanced.html":{"url":"golang/golang-advanced.html","title":"Golang进阶","keywords":"","body":"Golang面试题 1. 并发篇 1.1. 进程、线程、协程的区别 进程 进程是资源分配的单位，即资源隔离。同进程内的所有线程可以共同访问该进程的资源，每个进程都有自己的独立内存空间，不同进程通过进程间通信来通信。由于进程比较重量，占据独立的内存，所以上下文进程间的切换开销（栈、寄存器、虚拟内存、文件句柄等）比较大，但相对比较稳定安全。 线程 线程是独立调度和运行的基本单位，线程间通信主要通过共享内存，上下文切换很快，资源开销较少，但相比进程不够稳定容易丢失数据。 协程 协程是一种用户态的轻量级线程，协程的调度完全由用户控制。协程没有线程的上下文切换消耗， 1.2. 并发和并行的区别 并发（Concurrent）：并存的，一个CPU同时做多件事，实际是在不断地切换时间片去做多件事，看起来像是同时在做多件事。例如：CPU在时间片1中做任务A，时间片2中做任务B，时间片3中再做任务A，时间片4中再做任务B，不断在多个任务之间来回切换执行。 并行（Parallel）：平行的，多个CPU同时做多件事，CPU1做任务A，CPU2做任务B，不同CPU做不同时的任务，互不干扰。 并行由于有多个CPU分别做不同的任务，并不会互相抢占资源。并发是由一个CPU做多个任务，因此存在抢占资源，切换任务的操作。 2. 概念篇 2.1. defer defer（中文翻译：推迟）主要用来推迟执行，即在函数结束之前执行某些操作，其中函数结束主要包括return和panic两种情况，即正常结束和异常结束。 defer的执行顺序 defer在声明时不会执行，而是推迟执行，在return前执行，defer的执行顺序是先进后出，倒序依次执行不同的defer语句。 defer的特性 即使函数抛出异常，defer仍会被执行，这样不会出现程序错误导致资源不被释放，或者因为第三方包的异常导致程序崩溃。 defer的作用 用来释放资源（关闭文件等）、清理数据、记录日志、异常处理等，因此不管函数是否正常结束，都会在结束前逆序执行defer语句。 2.2. recover （recover）内建函数被用于从 panic 或 错误场景中恢复：让程序可以从 panicking 重新获得控制权，停止终止过程进而恢复正常执行。recover 只能在 defer 修饰的函数中使用：用于取得 panic 调用中传递过来的错误值，如果是正常执行，调用 recover 会返回 nil，且没有其它效果。 2.3. 闭包 参考： A Tour of Go 常用在go和defer中。 闭包是指引用了函数外的变量的一种函数，这样该函数就被绑定在某个变量上。 匿名函数同样被称之为闭包（函数式语言的术语）：它们被允许调用定义在其它环境下的变量。闭包可使得某个函数捕捉到一些外部状态，例如：函数被创建时的状态。另一种表示方式为：一个闭包继承了函数所声明时的作用域。这种状态（作用域内的变量）都被共享到闭包的环境中，因此这些变量可以在闭包中被操作，直到被销毁， 闭包是包含自由变量的代码块，这些变量不在这个代码块内或者任何全局上下文中定义，而是在定义代码块的环境中定义。由于自由变量包含在代码块中，所以只要闭包还被使用，那么这些自由变量以及它们引用的对象就不会被释放，要执行的代码为自由变量提供绑定的计算环境。 闭包的价值在于可以作为函数对象或者匿名函数，对于类型系统而言，这意味着不仅要表示数据还要表示代码。支持闭包的多数语言都将函数作为第一级对象，就是说这些函数可以存储到变量中作为参数传递给其他函数，最重要的是能够被函数动态创建和返回。 2.3.1. golang for range中的闭包 Frequently Asked Questions (FAQ) - The Go Programming Language CommonMistakes · golang/go Wiki · GitHub 在for range中goroutine的方式使用闭包，如果没有给匿名函数传入一个变量，或新建一个变量存储迭代的变量，那么goroutine执行的结果会是最后一个迭代变量的结果，而不是每个迭代变量的结果。这是因为如果没有通过一个变量来拷贝迭代变量，那么闭包因为绑定了变量，当每个groutine运行时，迭代变量可能被更改。 // false, print 3 3 3 values := []int{1,2,3} for _, val := range values { go func() { fmt.Println(val) }() } // true, print 1 2 3 for _, val := range values { go func(val interface{}) { fmt.Println(val) }(val) } 3. 原理篇 3.1. Goroutine调度模型 3.1.1. 简单介绍一下go的调度模型 参考：The Go scheduler - Morsing's blog 首先，Goroutine的调度是通过GPM模型来实现的。其中 G：goroutine，代表goroutine，即执行的goroutine的数据结构及栈等。（我们可以把它比喻成砖头） P：processor，代表调度执行的上下文（context），维护了一个本地的goroutine的队列。（我们可以把它比喻成小推车） M：machine，代表系统内核进程，用来执行G。（我们可以把它比喻成工人） 其次，调度的本质是将G尽量均匀合理地安给M来执行，其中P的作用就是来实现合理安排逻辑。 P的数量通过 GOMAXPROCS() 来设置，一般等于CPU的核数，对于一次代码执行设置好一般不会变。 P维护了一个本地的G队列，包括正在执行和待执行的G，尽量保证所有的P都匹配一个M同时在执行G。 P和M是动态形式的一对一的关系，P和G是动态形式的一对多的关系。 P存在的原因： 当goroutine发生阻塞的时候，可以通过P将剩余的G切换给新的M来执行，而不会导致剩余的G无法执行，如果没有M则创建M来匹配P。 P可以偷任务即goroutine，当某个P的本地G执行完，且全局没有G需要执行的时候，P可以去偷别的P还没有执行完的一半的G来给M执行，提高了G的执行效率。 3.1.2. GPM模型，为什么需要P? 没有P带来的问题： 早期Go中的调度器直接将G分配到合适的M上运行。但这样带来了很多问题，例如，不同的G在不同的M上并发运行时可能都需向系统申请资源（如堆内存），由于资源是全局的，将会由于资源竞争造成很多系统性能损耗。 P解决的问题 后面的Go（Go1.1）运行时系统加入了P，让P去管理G对象，M要想运行G必须先与一个P绑定，然后才能运行该P管理的G。这样带来的好处是，我们可以在P对象中预先申请一些系统资源（本地资源），G需要的时候先向自己的本地P申请（无需锁保护），如果不够用或没有再向全局申请，而且从全局拿的时候会多拿一部分，以供后面高效的使用。 就像现在我们去政府办事情一样，先去本地政府看能搞定不，如果搞不定再去中央，从而提供办事效率。 而且由于P解耦了G和M对象，这样即使M由于被其上正在运行的G阻塞住，其余与该M关联的G也可以随着P一起迁移到别的活跃的M上继续运行，从而让G总能及时找到M并运行自己，从而提高系统的并发能力。 Go运行时系统通过构造G-P-M对象模型实现了一套用户态的并发调度系统，可以自己管理和调度自己的并发任务，所以可以说Go语言原生支持并发。自己实现的调度器负责将并发任务分配到不同的内核线程上运行，然后内核调度器接管内核线程在CPU上的执行与调度。 3.2. 垃圾回收 3.2.1. 三色标记法 go的垃圾回收机制是通过三色标记法来实现的，其中 黑色：没有指向（引用）白色集合中的任何对象 灰色：可能指向（引用）白色集合中的某些对象 白色：剩下的需要被回收的候选对象，当灰色集合为空时，表示白色集合中的对象都没有被引用，那么这些对象就可以被回收。 一个垃圾回收循环的步骤： 将所有的对象都放入白色集合中 扫描所有roots对象，放入灰色集合中，roots对象表示在应用中可以被直接访问，一般是全局变量和其他在栈中的对象。 将灰色集合中的某个对象放入黑色集合，然后扫描这个对象有引到到的白色集合中的对象，将那些白色集合中引用到的所有对象放入灰色集合，以此类推，将灰色集合中的对象不断放入黑色集合中，然后白色集合中的对象不断放入灰色集合中。 当灰色集合中的对象为0，即都被放入到黑色集合中了，表示没有任何对象会引用到白色集合中的对象了，因为黑色集合存放不会引用白色集合对象的元素，而灰色集合为0，也不存在引用白色集合对象的元素。所以白色集合中的对象即是没有被引用的对象，可以回收的对象。 3.2.2. 三色标记和写屏障 这是让标记和⽤户代码并发的基本保障，基本原理： • 起初所有对象都是⽩⾊。 • 扫描找出所有可达对象，标记为灰⾊，放⼊待处理队列。 • 从队列提取灰⾊对象，将其引⽤对象标记为灰⾊放⼊队列，⾃⾝标记为⿊⾊。 • 写屏障监视对象内存修改，重新标⾊或放回队列。 当完成全部扫描和标记⼯作后，剩余不是⽩⾊就是⿊⾊，分别代表要待回收和活跃对象，清理操作只需将⽩⾊对象内存收回即可。 3.2.3. 标记清除算法 (Mark-Sweep) 标记-清除算法分为两个阶段：标记阶段和清除阶段。标记阶段的任务是标记出所有需要被回收的对象，清除阶段就是回收被标记的对象所占用的空间。 优点是简单，容易实现。 缺点是容易产生内存碎片，碎片太多可能会导致后续过程中需要为大对象分配空间时无法找到足够的空间而提前触发新的一次垃圾收集动作。（因为没有对不同生命周期的对象采用不同算法，所以碎片多，内存容易满，gc频率高，耗时） 3.3. 内存分配 3.3.1. 内存分配器的问题 当给不同大小的变量分配连续地址的内存的时候，可能因为部分变量内存的回收导致在分配新的内存需求时无法利用被回收的内存地址，因此内存管理不当，容易导致内存的碎片。 3.3.2.基本策略： 每次从操作系统申请⼀⼤块内存（⽐如 1MB），以减少系统调⽤。 将申请到的⼤块内存按照特定⼤⼩预先切分成⼩块，构成链表。 为对象分配内存时，只需从⼤⼩合适的链表提取⼀个⼩块即可。 回收对象内存时，将该⼩块内存重新归还到原链表，以便复⽤。 如闲置内存过多，则尝试归还部分内存给操作系统，降低整体开销 3.3.3.内存分配的本质 针对不同大小的对象，在不同的 cache 层中，使用不同的内存结构；将从系统中获得的一块连续内存分割成多层次的 cache，以减少锁的使用以提高内存分配效率；申请不同类大小的内存块来减少内存碎片，同时加速内存释放后的垃圾回收。 go的内存分配器将内存页分成67个不同大小规格（size class）的块，最小为8KB，最大为32768KB。 内存块的分类： span:由多个地址连续的页（page）组成的大块内存。面向内部管理。 object：将span按照特定的大小切分成多个小块，每个小块可以存储一个对象。面向对象分配。 内存分配器的三个数据结构（申请逐级向上）： mcache：goroutine cache，可以认为是 本地 cache。不涉及锁竞争。 mcentral：全局cache，mcache 不够用的时候向 mcentral 申请。涉及锁竞争。 mheap：当mcentral 也不够用的时候，通过 mheap 向操作系统申请。 3.3.4.内存分配的流程 object size object size > 32K，则使用 mheap 直接分配。 object size > 16K && object size 如果 mcache 对应的 size class 的 span 已经没有可用的块，则向 mcentral 请求。 如果 mcentral 也没有可用的块，则向 mheap申请，并切分。 如果 mheap 也没有合适的 span，则想操作系统申请。 3.3.5.内存回收的流程 mcache 归还内存分两部分：归还mcentral内存，可能涉及锁竞争；除此之外，归还到mheap，直接插入链表头。 mcentral 归还mheap。 mheap 定时归还系统内存。 3.3.6.tcmalloc(thread-caching mallo) 是google推出的一种内存分配器。 具体策略：全局缓存堆和进程的私有缓存。 1.对于一些小容量的内存申请试用进程的私有缓存，私有缓存不足的时候可以再从全局缓存申请一部分作为私有缓存。 2.对于大容量的内存申请则需要从全局缓存中进行申请。而大小容量的边界就是32k。缓存的组织方式是一个单链表数组，数组的每个元素是一个单链表，链表中的每个元素具有相同的大小。 golang语言中MHeap就是全局缓存堆，MCache作为线程私有缓存。 内存池就是利用MHeap实现，大小切分则是在申请内存的时候就做了，同时MCache分配内存时，可以用MCentral去取对应的sizeClass，多线程管理方面则是通过MCache去实现。 总结: 1.MHeap是一个全局变量，负责向系统申请内存，mallocinit()函数进行初始化。如果分配内存对象大于32K直接向MHeap申请。 2.MCache线程级别管理内存池，关联结构体P，主要是负责线程内部内存申请。 3.MCentral连接MHeap与MCache的，MCache内存不够则向MCentral申请，MCentral不够时向MHeap申请内存。 3.4. channel的原理 channel的作用是解决goroutine之间的通信问题。不要通过共享内存来通信，而应该通过通信来共享内存。 3.4.1. channel的特性 goroutine安全 提供FIFO语义(buffered channel提供)，缓冲大小 在不同的goroutine之间存储和传输值 可以让goroutine block/unblock 3.4.2. channel的数据结构 type hchan struct { qcount uint // 当前队列中剩余元素个数 dataqsiz uint // 环形队列长度，即可以存放的元素个数 buf unsafe.Pointer // 环形队列指针 elemsize uint16 // 每个元素的大小 closed uint32 // 标识关闭状态 elemtype *_type // 元素类型 sendx uint // 队列下标，指示元素写入时存放到队列中的位置 recvx uint // 队列下标，指示元素从队列的该位置读出 recvq waitq // 等待读消息的goroutine队列 sendq waitq // 等待写消息的goroutine队列 lock mutex // 互斥锁，chan不允许并发读写 } hchan维护了两个链表，recvq是因读这个chan而阻塞的G，sendq则是因写这个chan而阻塞的G。waitq队列中每个元素的数据结构为sudog，其中elem用于保存数据。 3.4.3. 创建channel make函数在创建channel的时候会在该进程的heap区申请一块内存，创建一个hchan结构体，返回执行该内存的指针，所以获取的的ch变量本身就是一个指针，在函数之间传递的时候是同一个channel。 hchan结构体使用一个环形队列来保存groutine之间传递的数据(如果是缓存channel的话)，使用两个list保存像该chan发送和从改chan接收数据的goroutine，还有一个mutex来保证操作这些结构的安全。 3.4.4. 写入channel recvq存放读取channel阻塞的G，此时说明channel里面没有数据。sendq存放写入channel阻塞的G，此时说明channel已经满了。 如果等待接收队列recvq不为空，说明缓冲区中没有数据或者没有缓冲区，此时直接从recvq取出G,并把数据写入，最后把该recvq的G唤醒，结束发送过程； 如果缓冲区中有空余位置，将数据写入缓冲区，结束发送过程； 如果缓冲区满了，将待发送数据写入G，将当前G加入sendq，进入睡眠（阻塞状态），等待被读goroutine唤醒； 3.4.5. 读出channel 如果等待发送队列sendq不为空，且没有缓冲区，直接从sendq中取出G，把G中数据读出，最后把G唤醒，结束读取过程； 如果等待发送队列sendq不为空，此时说明缓冲区已满，从缓冲区中首部读出数据，把G中数据写入缓冲区尾部，把G唤醒，结束读取过程； 如果等待发送队列sendq为空，说明缓冲区中有数据，则从缓冲区取出数据，结束读取过程； 如果channel读取不到数据，将当前goroutine加入recvq，进入睡眠（阻塞状态），等待被写goroutine唤醒； 3.4.6. 关闭channel 将 c.closed 设置为 1 唤醒 recvq 队列里面的阻塞 goroutine 唤醒 sendq 队列里面的阻塞 goroutine 3.4.7. 阻塞 当G1向buf已经满了的ch发送数据的时候，当runtine检测到对应的hchan的buf已经满了，会通知调度器，调度器会将G1的状态设置为waiting, 移除与线程M的联系，然后从P的runqueue中选择一个goroutine在线程M中执行，此时G1就是阻塞状态，但是不是操作系统的线程阻塞，所以这个时候只用消耗少量的资源。 3.4.8. 唤醒 当G1变为waiting状态后，会创建一个代表自己的sudog的结构，然后放到sendq这个list中，sudog结构中保存了channel相关的变量的指针。当G2从ch中接收一个数据时，会通知调度器，设置G1的状态为runnable，然后将加入P的runqueue里，等待线程执行. 4. 源码篇 4.1. 切片的实现原理 4.2. map的实现原理 4.3. channel为什么是线程安全的？ Copyright © www.huweihuang.com all right reserved，powered by GitbookUpdated at 2025-07-08 20:55:40 "},"k8s/k8s-basic.html":{"url":"k8s/k8s-basic.html","title":"k8s基础","keywords":"","body":"k8s网络 访问一个pod的ip例如10.232.46.109，那么会先经过代理机器，在代理机器上查看路由表 $ route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 10.232.46.64 192.168.99.42 255.255.255.192 UG 0 0 0 bond0 发现要转发给网关192.168.99.42，所以包转发到节点机器192.168.99.42 查看192.168.99.42的路由表 $ route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 10.232.46.109 0.0.0.0 255.255.255.255 UH 0 0 0 calia7b9a112dad 发现可以到本地的10.232.46.109网卡。 同节点的pod通信 在pause容器启动之前，会创建一个虚拟以太网接口对（veth pair），该接口对一端连着容器内部的eth0 ，一端连着容器外部的vethxxx，vethxxx会绑定到容器运行时配置使用的网桥bridge0上，从该网络的IP段中分配IP给容器的eth0。 当同节点上的Pod-A发包给Pod-B时，包传送路线如下： pod-a的eth0—>pod-a的vethxxx—>bridge0—>pod-b的vethxxx—>pod-b的eth0 因为相同节点的bridge0是相通的，因此可以通过bridge0来完成不同pod直接的通信，但是不同节点的bridge0是不通的，因此不同节点的pod之间的通信需要将不同节点的bridge0给连接起来。 不同节点的pod通信 连接不同节点的bridge0的方式有好几种，主要有overlay和underlay，或常规的三层路由。 不同节点的bridge0需要不同的IP段，保证Pod IP分配不会冲突，节点的物理网卡eth0也要和该节点的网桥bridge0连接。因此，节点a上的pod-a发包给节点b上的pod-b，路线如下： 节点a上的pod-a的eth0—>pod-a的vethxxx—>节点a的bridge0—>节点a的eth0—> 节点b的eth0—>节点b的bridge0—>pod-b的vethxxx—>pod-b的eth0 其中一个关键点是，需要有Pod IP的路由表，这一点可以通过calico解决。 源码分析-kubelet 初始化各种所需参数，创建并初始化kubelet结构体，运行kubelet.run方法， kubelet通过各种类型的manager异步工作各自执行各自的任务，例如podManager负责pod生命周期的管理。 其中使用到了多种的channel来控制状态信号变化的传递，例如比较重要的channel有podUpdates ，来传递pod的变化情况。 其中pod的管理使用了syncPod的函数来处理pod相关的操作，例如创建sandbox，启动init容器，拉取镜像，启动容器，执行post start hook。 Service Service定义了一种抽象：一组Pod的访问逻辑抽象，这组Pod可以理解为一组微服务，而访问该组微服务只需要访问Service这个入口，而无需关心这组微服务对应了多少个实例和实例的变化（例如pod被重建后IP变化的问题）。 Service与Pod之间的关联通过Label Selector的方式实现。Service解耦了fronend与backend之间的直接联系，而是由Service来作为fronend和backend之间通信的桥梁。 对 Kubernetes 集群中的应用，Kubernetes 提供了简单的 Endpoints API，只要 Service 中的一组 Pod 发生变更，应用程序就会被更新。 对非 Kubernetes 集群中的应用，Kubernetes 提供了基于 VIP 的网桥的方式访问 Service，再由 Service 重定向到 backend Pod。 参考：https://k8smeetup.github.io/docs/concepts/services-networking/service/ kube-proxy的工作原理 kube-proxy本质就是实现service对应的一组pod的流量代理功能，管理service和endpoint的对应关系，根据endpoint的变化在每个节点上生成对应的iptables的规则。 当访问service cluster ip和端口时，iptables的规则会通过NAT的方式将流量转到service对应的pod上，默认是轮询的方式访问。 缺点是当endpoint特别多的时候，iptables rules将会非常庞大，则存在性能问题。 所以目前大部分是通过ingress controller来实现流量代理而不是kube-proxy的方式。 ingress的工作原理 ingress是k8s的资源对象，描述了一个或多个的路由规则。而ingress controller实时实现对应的路由规则，ingress controller常用的有Nginx ingress controller。 基本逻辑是当新增一个pod的时候，有ingress自动维护对应的路由规则，ingress controller实时更新Nginx的配置，并reload Nginx使其生效。 客户端首先对 kubia.example.com 执行 DNS 查 找， DNS 服务器（或本地操作系统）返回了Ingress 控制器的 IP。客户端然后 向 Ingress 控制器发送 HTTP 请求，并在 Host 头中指定 kubia . example.com。控制器从该头部确定客户端尝试访 问哪个服务，通过与该服务关联 的 Endpoint 对象查看 pod IP ，并将客户端的请求转发给其中一个pod 。 如你所见， Ingress 控制器不会将请求转发给该服务，只用它来选择一个 pod 。大多数（即使不是全部）控制器都是这样工作的 。 各组件的原理 apiserver apiserver提供了集群各类资源管理的API接口，apiserver是无状态的，将全部的数据状态保存在etcd中，只有apiserver与etcd交互，其他组件 controller-manager、scheduler、kubelet通过apiserver来进行交互。 Controller-manager controller-manager是各种控制器的集合，它的功能是使集群资源的状态始终保持在用户期望的状态。 scheduler 预选 全局选可：先根据预选的策略从全局中选出符合要求的候选节点。 预选策略有 PodFitsResources：判断节点资源是否满足 PodSelectorMatches：是否有指定label或taint，即nodeselector PodFitsHost：是否有指定节点调度 选优 从符合条件的候选节点中选择最优的节点。 优选策略有 LeastRequestedPriority：选择资源消耗最小的节点 BalancedResourceAllocation：选择资源使用率最均衡的节点 NodeAffinity：节点亲缘性或反亲缘性调度 具体操作如下： PrioritizeNodes通过并行运行各个优先级函数来对节点进行优先级排序。 每个优先级函数会给节点打分，打分范围为0-10分。 0 表示优先级最低的节点，10表示优先级最高的节点。 每个优先级函数也有各自的权重。 优先级函数返回的节点分数乘以权重以获得加权分数。 最后组合（添加）所有分数以获得所有节点的总加权分数。 抢占 当pod不适合任何节点的时候，可能pod会调度失败。这时候可能会根据pod的优先级发生抢占。 kubelet kubelet主要用来管理分配到该节点上的pod的生命周期，定期向apiserver汇报节点的健康状况。 list-watch机制 Etcd 存储集群的数据信息，apiserver 作为统一入口，任何对数据的操作都必须经过 apiserver。客户端(kubelet/scheduler/ontroller-manager)通过 list-watch 监听 apiserver 中资源(pod/rs/rc 等等)的 create, update 和 delete 事件，并针对事件类型调用相应的事件处理函数。 那么 list-watch 具体是什么呢，顾名思义，list-watch 有两部分组成，分别是 list 和 watch。list 非常好理解，就是调用资源的 list API 罗列资源，基于 HTTP 短链接实现；watch 则是调用资源的 watch API 监听资源变更事件，基于 HTTP 长链接实现，也是本文重点分析的对象。 首先消息必须是可靠的，list 和 watch 一起保证了消息的可靠性，避免因消息丢失而造成状态不一致场景。具体而言，list API 可以查询当前的资源及其对应的状态(即期望的状态)，客户端通过拿期望的状态和实际的状态进行对比，纠正状态不一致的资源。Watch API 和 apiserver 保持一个长链接，接收资源的状态变更事件并做相应处理。如果仅调用 watch API，若某个时间点连接中断，就有可能导致消息丢失，所以需要通过 list API 解决消息丢失的问题。从另一个角度出发，我们可以认为 list API 获取全量数据，watch API 获取增量数据。虽然仅仅通过轮询 list API，也能达到同步资源状态的效果，但是存在开销大，实时性不足的问题。 informer的组件 Controller：用来把Reflector、DeltaFIFO组合起来形成一个相对固定的、标准的处理流程。 Reflector：通过 Kubernetes Watch API 监听某种 resource 下的所有事件 DeltaFIFO：记录对象变化的先进先出的队列 LocalStore：二级缓存，LocalStore 只会被 Lister 的 List/Get 方法访问 。与DeltaFIFO存在resync。 Lister：调用 List/Get 方法 Processor：记录了所有的回调函数实例(即 ResourceEventHandler 实例)，并负责触发这些函数。 流程： Informer 在初始化时，Reflector 会先 List API 获得所有的 Pod Reflect 拿到全部Pod后，会将全部 Pod 放到 Store 中 如果有人调用 Lister 的 List/Get 方法获取 Pod， 那么 Lister 会直接从 Store 中拿数据 Informer 初始化完成之后，Reflector 开始 Watch Pod，监听 Pod 相关 的所有事件;如果此时 pod_1 被删除，那么 Reflector 会监听到这个事件 Reflector 将 pod_1 被删除 的这个事件发送到 DeltaFIFO DeltaFIFO 首先会将这个事件存储在自己的数据结构中(实际上是一个 queue)，然后会直接操作 Store 中的数据，删除 Store 中的 pod_1 DeltaFIFO 再 Pop 这个事件到 Controller 中 Controller 收到这个事件，会触发 Processor 的回调函数，回调函数主要有OnAdd、OnUpdate和 OnDelete三个方法。 流程2： reflector通过list-watch的机制将对象添加到DeltaFIFO的队列中，ListAndWatch第一次会列出所有的对象，并获取资源对象的版本号，然后watch资源对象的版本号来查看是否有被变更。list()可能会导致本地的缓存相对于etcd里面的内容存在延迟，Reflector会通过watch的方法将延迟的部分补充上，使得本地的缓存数据与etcd的数据保持一致。 Reflector的主要作用是watch指定的k8s资源，并将变化同步到本地是store中。Reflector以resyncPeriod为周期定期执行list的操作，这样就可以使用Reflector来定期处理所有的对象，也可以逐步处理变化的对象。 controller.Run函数还会调用processLoop函数，processLoop通过调用HandleDeltas，再调用distribute，processorListener.add最终将不同更新类型的对象加入processorListener的channel中，供processorListener.Run使用。 processor的主要功能就是记录了所有的回调函数实例(即 ResourceEventHandler 实例)，并负责触发这些函数。processor记录了不同类型的事件函数，其中事件函数在NewXxxController构造函数部分注册，具体事件函数的处理，一般是将需要处理的对象加入对应的controller的任务队列中，然后由类似syncDeployment的同步函数来维持期望状态的同步逻辑。 Pod的流程 通过kubectl部署一个应用， apiserver接收到请求，并将数据存放到etcd中， controller发现有数据变化，调用对应的controller去创建期望的pod， scheduler通过计算选出适合该pod的最优的节点 节点上的kubelet发现需要创建pod，就通过container的接口去部署pod Kube-proxy管理pod的网络转发，包括服务发现和负载均衡。 详细流程： 流程1(RC创建)用户通过WebUI或者kubectl工具调用kube-apiserver提供的标准 REST API接口创建一个ReplicationController。假定在ReplicationController数据结构中定义为2个Pod。用户相当执行了下面的 curl命令。 curl -XPOST -d \"v1.ReplicationController\" -H \"Content-Type: application/json\" http://ip:port/api/v1/namespaces/{namespace}/replicationcontrollers 而此处的v1.ReplicationController即用户的声明式数据，其中会指定应用的实例数，使用镜像等信息 流程2~3(Pod创建)kube-controller-manager会通过list-watch机制获取到新建中的 v1.ReplicationController数据，并驱使ReplicationController控制器工作。 kube-controller-manager中各个控制器模块的工作就是让集群现状趋于用户期待。假定用户声明实例是2，而当前集群中对应的Pod数量为0，则控制器就会创建2个Pod. 当然后续controller-manager也会通过list-watch机制获取到新创建的两个Pod，因为和用户期待值一致，ReplicationController的控制逻辑就收敛在用户期待状态了。 流程4~5(Pod调度)kube-scheduler也会通过list-watch机制获取到新创建的 v1.Pod，然后根据调度算法为Pod选择最合适的节点。假定调度结果是两个Pod选择的是minion1和minion2.即刷新v1.Pod数据的 spec.nodeName字段为minion1和minion2的nodeName。 流程6~7(Pod运行)最后minion上的kubelet也会通过list-watch机制获取到调度完的 v1.Pod数据，然后通过container.Runtime接口创建Pod中指定的容器。并刷新ETCD中Pod的运行状态。 运行流程看Pod状态数据变化根据上述的介绍，在运行过程中Pod状态变化 PV及PVC的流程 未使用sc的步骤： 管理员创建某类型的网络存储，如nfs 管理员创建pv使用该网络存储 用户创建一个pvc绑定该pv 用户创建pod引用该pvc pv不属于任何namespace，是属于集群级别的资源。 使用storageclass的步骤流程： 管理员部署持久卷置配程序，即provisioner 管理员创建一个使用了该provisioner的storageclass 用户创建一个引用了该storageclass的pvc k8s查找其引用的storageclass，并根据pvc中指定的访问模式和存储大小在storageclass配置新的pv Provisioner创建对应的pv，pv和pvc形成绑定关系 创建一个pod，引用该pvc。 pause容器的作用 1.是提供Pod在Linux中共享命名空间的基础， 2.是提供Pid Namespace并使用init进程。 Copyright © www.huweihuang.com all right reserved，powered by GitbookUpdated at 2025-07-08 20:55:40 "},"k8s/k8s-advanced.html":{"url":"k8s/k8s-advanced.html","title":"k8s进阶","keywords":"","body":"kubernetes面试题 Pod的创建整体流程 kubelet上创建pod的流程 list-watch机制 为什么k8s的节点控制在5000台，瓶颈在哪里 如何实现Pod固定IP 如何实现原地升级 如何实现动态修改容器资源且不重启 怎么解决 k8s基于pod request 调度导致与实际节点负载不一致问题 Copyright © www.huweihuang.com all right reserved，powered by GitbookUpdated at 2025-07-08 20:55:40 "},"leetcode/leetcode.html":{"url":"leetcode/leetcode.html","title":"LeetCode","keywords":"","body":"Copyright © www.huweihuang.com all right reserved，powered by GitbookUpdated at 2025-07-08 20:55:40 "}}